main = paste0("Degree = 1; Error = ", round(error1, digits = 4)))
## reproduce figure 4.3 left
?plot
abline(coef(m1), col = "orange")
abline(coef(ls1), col = "orange")
abline(coef(ls2), col = "blue")
abline(coef(ls3), col = "green")
plot(0, 0, type = "n",
xlim = c(0, 1), ylim = c(0,1), xlab = "X.proj", ylab = "Y",
main = paste0("Degree = 1; Error = ", round(error1, digits = 4)))
abline(coef(ls1), col = "red")
abline(coef(ls2), col = "green")
abline(coef(ls3), col = "blue")
points(X.proj, fitted(ls1), pch="1", col="red")
points(X.proj, fitted(ls2), pch = "2", col = "green")
points(X.proj, fitted(ls3), pch = "3", col = "blue")
plot(C1[,1],C1[,2],pch='1',col = 'orange',
xlim = c(min(C1[,1]),max(C3[,1])), ylim=c(min(C1[,2]),max(C3[,2])))
points(C2[,1],C2[,2], pch='2',col = 'lightblue')
points(C3[,1],C3[,2], pch='3', col = 'darkgreen')
## reproduce figure 4.3 left
# png()
# jpeg()
# bmp()
png("fig-4-3-left.png")
plot(0, 0, type = "n",
xlim = c(0, 1), ylim = c(0,1), xlab = "X.proj", ylab = "Y",
main = paste0("Degree = 1; Error = ", round(error1, digits = 4)))
abline(coef(ls1), col = "orange")
abline(coef(ls2), col = "lightblue")
abline(coef(ls3), col = "darkgreen")
plot(0, 0, type = "n",
xlim = c(0, 1), ylim = c(0,1), xlab = "X.proj", ylab = "Y",
main = paste0("Degree = 1; Error = ", round(error1, digits = 4)))
dev.off()
## reproduce figure 4.3 left
# png()
# jpeg()
# bmp()
png("fig-4-3-left.png")
plot(0, 0, type = "n",
xlim = c(0, 1), ylim = c(0,1), xlab = "X.proj", ylab = "Y",
main = paste0("Degree = 1; Error = ", round(error1, digits = 4)))
abline(coef(ls1), col = "orange")
abline(coef(ls2), col = "lightblue")
dev.off()
plot(0, 0, type = "n",
xlim = c(0, 1), ylim = c(0,1), xlab = "X.proj", ylab = "Y",
main = paste0("Degree = 1; Error = ", round(error1, digits = 4)))
mu_s <- c(0.25, 0.5, 0.75)
sigma_s <- 0.005*matrix(c(1,0,0,1),2,2)
library(MASS)
set.seed(872)
N <- 100
C1 <- mvrnorm(n = N, c(mu_s[1],mu_s[1]),Sigma = sigma_s)
C2 <- mvrnorm(n = N, c(mu_s[2],mu_s[2]),Sigma = sigma_s)
C3 <- mvrnorm(n = N, c(mu_s[3],mu_s[3]),Sigma = sigma_s)
# Figure 4.3
## project X onto the line joining the three centroids
X <- rbind(C1,C2,C3)
# project the data onto the line joining the three centroids
# then the length = sqrt(2)/2 * (X_1 + X_2)
X.proj <- sqrt(2)*rowMeans(X) # if necessary, multiply sqrt 2
y1 <- c(rep(1,N),rep(0,2*N))
y2 <- c(rep(0,N),rep(1,N),rep(0,N))
y3 <- c(rep(0,2*N),rep(1,N))
ls1 <- lm(y1~X.proj)
ls2 <- lm(y2~X.proj)
ls3 <- lm(y3~X.proj)
# order the corresponding prediction values y_hat
pred1 <- as.numeric(fitted(ls1)[order(X.proj)])
pred2 <- as.numeric(fitted(ls2)[order(X.proj)])
pred3 <- as.numeric(fitted(ls3)[order(X.proj)])
# get the first estimated class : 1 ~ c1
c1 <- which(pred1 <= pred2)[1]
# get the second estimated class: c1+1 ~ c2
c2 <- min(which(pred3 > pred2))
# get the third estimated class : c2 ~ c3
# but, you will find that c1 = c2 !!!
error1 <- (abs(c2 - 2*N) + abs(c1 - N))/(3*N)
## reproduce figure 4.3 right
lm1 <- lm(y1 ~ X.proj + I(X.proj^2))
lm2 <- lm(y2 ~ X.proj + I(X.proj^2))
lm3 <- lm(y3 ~ X.proj + I(X.proj^2))
# order the corresponding prediction values y_hat
pred21 <- as.numeric(fitted(lm1)[order(X.proj)])
pred22 <- as.numeric(fitted(lm2)[order(X.proj)])
pred23 <- as.numeric(fitted(lm3)[order(X.proj)])
# get the first estimated class : 1 ~ c21
c21 <- which(pred21 <= pred22)[1]
# get the second estimated class: c21+1 ~ c22
c22 <- max(which(pred23 <= pred22))
# get the third estimated class : c22 ~ end
error2 <- (abs(c22 - 2*N) + abs(c21 - N))/(3*N)
plot(0, 0, type = "n",
xlim = c(0, 1), ylim = c(0,1), xlab = "X.proj", ylab = "Y",
main = paste0("Degree = 2; Error = ", round(error2, digits = 4)))
abline(coef(lm1), col = "orange")
lines(sort(X.proj), pred21, col = "orange", type = 'o', pch = '1')
plot(0, 0, type = "n",
xlim = c(0, 1), ylim = c(-1,2),  xlab = "X.proj", ylab = "Y",
main = paste0("Degree = 2; Error = ", round(error2, digits = 4)))
lines(sort(X.proj), pred21, col = "orange", type = 'o', pch = '1')
lines(sort(X.proj), pred22, type = 'o', pch = '1', col = "lightblue")
plot(0, 0, type = "n",
xlim = c(0, 2), ylim = c(-1,2),  xlab = "X.proj", ylab = "Y",
main = paste0("Degree = 2; Error = ", round(error2, digits = 4)))
lines(sort(X.proj), pred21, type = 'o', pch = '1', col = "orange")
lines(sort(X.proj), pred22, type = 'o', pch = '1', col = "lightblue")
plot(0, 0, type = "n",
xlim = c(0, 2), ylim = c(-1,2),  xlab = "X.proj", ylab = "Y",
main = paste0("Degree = 2; Error = ", round(error2, digits = 4)))
lines(sort(X.proj), pred21, type = 'o', pch = '1', col = "orange")
lines(sort(X.proj), pred22, type = 'o', pch = '2', col = "lightblue")
lines(sort(X.proj), pred22, type = 'o', pch = '3',  col = "darkgreen")
plot(0, 0, type = "n",
xlim = c(0, 2), ylim = c(-1,2),  xlab = "X.proj", ylab = "Y",
main = paste0("Degree = 2; Error = ", round(error2, digits = 4)))
lines(sort(X.proj), pred21, type = 'o', pch = '1', col = "orange")
lines(sort(X.proj), pred22, type = 'o', pch = '2', col = "lightblue")
lines(sort(X.proj), pred32, type = 'o', pch = '3',  col = "darkgreen")
lines(sort(X.proj), pred23, type = 'o', pch = '3',  col = "darkgreen")
rug(X.proj[1:N], col = "orange")
rug(X.proj[(N+1):(2*N)], col = "lightblue")
rug(X.proj[(2*N+1):(3*N)], col = "darkgreen")
abline(h=c(0.0, 0.5, 1.0), lty=5, lwd = 0.4)
abline(v=c(sort(X.proj)[N], sort(X.proj)[N*2]), lwd = 0.4)
# plot
png("fig-4-3-right.png")
plot(0, 0, type = "n",
xlim = c(0, 2), ylim = c(-1,1),  xlab = "X.proj", ylab = "Y",
main = paste0("Degree = 2; Error = ", round(error2, digits = 4)))
lines(sort(X.proj), pred21, type = 'o', pch = '1', col = "orange")
lines(sort(X.proj), pred22, type = 'o', pch = '2', col = "lightblue")
lines(sort(X.proj), pred23, type = 'o', pch = '3',  col = "darkgreen")
rug(X.proj[1:N], col = "orange")
rug(X.proj[(N+1):(2*N)], col = "lightblue")
rug(X.proj[(2*N+1):(3*N)], col = "darkgreen")
abline(h=c(0.0, 0.5, 1.0), lty=5, lwd = 0.4)
abline(v=c(sort(X.proj)[N], sort(X.proj)[N*2]), lwd = 0.4)
dev.off()
# Logistic Regression
# Newton-Raphson Algorithm
setwd("~/Documents/Programming/machinelearning/Machine_Learning/ESL-Practice")
sa <- read.table('SAheart.data.txt', sep = ',', header = 1)
# or
#sa <- read.table("http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data",
#                 sep=",",head=T,row.names=1)
sa <- sa[,c(-1,-5,-7)]
sa$chd <- as.factor(sa$chd)
str(sa)
sa_stddz <- apply(sa[,c(-4, -8)],standardize,2)
?apply
sa_stddz <- apply(sa[c(-4, -8)],2,standardize)
sa_stddz <- apply(sa[c(-4, -8)],2,scale)
str(sa_stddz)
sa_stddz[1:3,1:3]
sa_stddz <- apply(sa[,c(-4, -8)],2,scale)
str(sa_stddz)
sa_stddz <- cbind(sa_stddz,sa[,c(4,8)])
str(sa_stddz)
# logistic regression of standardized data
lr_stdd_all <- glm(chd ~ .,
family = binomial(link = logit), data = sa_stddz)
summary(lr_stdd_all)
# logistic regression of standardized data
pairs(sa[,c(-4, -8)])
# logistic regression of standardized data
pairs(sa_stddz[,c(-7, -8)])
# logistic regression of standardized data
pairs(sa_stddz[,c(-7, -8)])
cor(sa_stddz[,c(-7, -8)])
cor(sa_stddz[,c(-7, -8)])
# L1 Regulized Logistic Regression
library(glmpath) # For the LASSO
require(glmnet)
library(lars) # Used for the LASSO
X <- sa[,-8]
# Make sure that family history is numeric
X$famhist <- as.numeric(X$famhist)
# Ensure that the explanatory variables are in a matrix form
X <- as.matrix(X)
# Ensure that the respones variable is numeric and in matrix form
y <- sa[,8]
y <- as.matrix(as.numeric(y))
SA_LARS <- glmpath(X, y, family=binomial,
standardize = T, max.norm=100000*ncol(X),
min.lambda = 0.1,trace = T)
summary(SA_LARS)
SA_LARS <- glmpath(X, y, family=binomial,
standardize = T, max.norm=100000*ncol(X),
min.lambda = 0.1)
# Standardized
X_st <- sa_stddz[,-8]
# Make sure that family history is numeric
X_st$famhist <- as.numeric(X_st$famhist)
# Ensure that the explanatory variables are in a matrix form
X_st <- as.matrix(X_st)
# Ensure that the respones variable is numeric and in matrix form
y_st <- sa_stddz[,8]
y_st <- as.matrix(as.numeric(y_st))
SA_LARS <- glmpath(X_st, y_st, family=binomial,
max.norm=100000*ncol(X),
min.lambda = 0.1,trace = T)
# Create data
genXY <- function(p = 0.5,  # correlation
N = 100, # number of sample
beta = c(4, 2)) # true coefficient
{
# covariance matrix
Sigma = matrix(c(1, p,
p, 1), 2, 2)
library(MASS)
X = mvrnorm(N, c(0, 0), Sigma)
Y = X[, 1] * beta[1] + X[, 2] * beta[2]
return(list(X=X, Y=Y))
}
# I. Least Squared
ols.fit = lm(Y ~ 0 + X)
# Create data
# From the statement of Figure 3.18
# beta_1 = 4; beta_2 = 2 --> beta = c(4, 2)
# top panel: rho = 0.5
# bottom panel: rho = -0.5
library(MASS)
rho = 0.5  # correlation
n = 100 # number of sample
beta = c(4, 2) # coefficient
Sigma = matrix(c(1, p,
p, 1), 2, 2)
dim(beta)
beta <- vector(4, 2) # coefficient
beta <- as.vector(c(4, 2)) # coefficient
dim(beta)
Y <- X %*% beta
X <- mvrnorm(N, c(0, 0), Sigma)
Sigma <- matrix(c(1, rho,
rho, 1), 2, 2)
X <- mvrnorm(N, c(0, 0), Sigma)
Y <- X %*% beta
X <- mvrnorm(n, c(0, 0), Sigma)
Y <- X %*% beta
# Create data
# From the statement of Figure 3.18
# beta_1 = 4; beta_2 = 2 --> beta = c(4, 2)
# top panel: rho = 0.5
# bottom panel: rho = -0.5
library(MASS)
rho <- 0.5  # correlation
n <- 100 # number of sample
beta <- as.vector(c(4, 2)) # coefficient
Sigma <- matrix(c(1, rho,
rho, 1), 2, 2)
X <- mvrnorm(n, c(0, 0), Sigma)
Y <- X %*% beta
Y.s <- X[, 1] * beta[1] + X[, 2] * beta[2]
# I. Least Squared
Y - Y.s
# IV. PCA
# Using package: pls
library(pls)
expression(beta[1])
# Final: Draw the Graph
plot(0, 0,
xlab = expression(beta[1]),
ylab = expression(beta[2]),
main = substitute(paste(rho,"=",r), list(r=rho)),
xlim = c(0, 6),
ylim = c(-1, 3),
type = "n")
par(lwd = 3, cex = 1)
lines(ridge_beta, col = "red")
# III. Ridge
ridge_fit = glmnet(X, Y, alpha = 0, lambda = grid_search)
# II. Lasso
# Using package glmnet
# Try different lamdas
grid_search = 10^seq(10, -2, length = 100)
library(glmnet)
lasso_fit = glmnet(X, Y, alpha = 1, lambda = grid_search)
lasso_beta = as.matrix(lasso_fit$beta)
lasso_beta = t(lasso_beta)
# III. Ridge
ridge_fit = glmnet(X, Y, alpha = 0, lambda = grid_search)
ridge_beta = as.matrix(ridge_fit$beta)
ridge_beta = t(ridge_beta)
lines(ridge_beta, col = "red")
lines(lasso_beta, col = "green")
# IV. PCR
# Using package: pls
library(pls)
pcr_fit = pcr(Y ~ X, scale = FALSE)
pcr_beta = pcr_fit$coefficients
pcr_beta = rbind(c(0, 0), pcr_beta[,,1], pcr_beta[,,2])
# V. PLS
pls_fit = plsr(Y ~ X, scale = FALSE)
pls_beta = pls_fit$coefficients
pls_beta = rbind(c(0, 0), pls_beta[,,1], pls_beta[,,2])
# VI. Best Subset
# Using package leaps
library(leaps)
bs_fit = regsubsets(x = X, y = Y, intercept = FALSE)
if (summary(bs_fit)$which[1, 1])
{
bs_beta = c(coef(bs_fit, 1), 0)
} else {
bs_beta = c(0, coef(bs_fit, 1))
}
bs_beta = rbind(c(0, 0), bs_beta, coef(bs_fit, 2))
points(ls_beta, col = "black", pch = 16)
# I. Least Squared
ls_fit <- lm(Y ~ 0 + X)
ls_beta <- coef(ols_fit)
ls_beta <- coef(ls_fit)
ls_beta <- as.matrix(t(ls_beta))
points(ls_beta, col = "black", pch = 16)
lines(pcr_beta, col = "purple")
lines(pls_beta, col = "orange")
lines(subset_beta, col = "blue")
lines(bs_beta, col = "blue")
abline(h=0, lty = 2)
abline(v=0, lty = 2)
legend(4.8, 3,
c("Ridge", "Lasso", "PCR", "PLS", "Best Subset", "Least Squares"),
col = c("red", "green", "purple", "orange", "blue", "black"),
lty = c(1,1,1,1,1,NA),
pch =c(NA,NA,NA,NA,NA, 16),
box.col = "white",
box.lwd = 0,
bg = "transparent")
# When rho = -0.5
rho <- -0.5  # correlation
n <- 100 # number of sample
beta <- as.vector(c(4, 2)) # coefficient
Sigma <- matrix(c(1, rho,
rho, 1), 2, 2)
X <- mvrnorm(n, c(0, 0), Sigma)
Y <- X %*% beta
# I. Least Squared
ls_fit <- lm(Y ~ 0 + X)
ls_beta <- coef(ls_fit)
ls_beta <- as.matrix(t(ls_beta))
# II. Lasso
# Using package glmnet
# Try different lamdas
grid_search = 10^seq(10, -2, length = 100)
library(glmnet)
lasso_fit = glmnet(X, Y, alpha = 1, lambda = grid_search)
lasso_beta = as.matrix(lasso_fit$beta)
lasso_beta = t(lasso_beta)
# III. Ridge
ridge_fit = glmnet(X, Y, alpha = 0, lambda = grid_search)
ridge_beta = as.matrix(ridge_fit$beta)
ridge_beta = t(ridge_beta)
# IV. PCR
# Using package: pls
library(pls)
pcr_fit = pcr(Y ~ X, scale = FALSE)
pcr_beta = pcr_fit$coefficients
pcr_beta = rbind(c(0, 0), pcr_beta[,,1], pcr_beta[,,2])
# V. PLS
pls_fit = plsr(Y ~ X, scale = FALSE)
pls_beta = pls_fit$coefficients
pls_beta = rbind(c(0, 0), pls_beta[,,1], pls_beta[,,2])
# VI. Best Subset
# Using package leaps
library(leaps)
bs_fit = regsubsets(x = X, y = Y, intercept = FALSE)
if (summary(bs_fit)$which[1, 1])
{
bs_beta = c(coef(bs_fit, 1), 0)
} else {
bs_beta = c(0, coef(bs_fit, 1))
}
bs_beta = rbind(c(0, 0), bs_beta, coef(bs_fit, 2))
if (summary(bs_fit)$which[1, 1])
if (summary(bs_fit)$which[1, 1]){
bs_beta = c(coef(bs_fit, 1), 0)
} else {
bs_beta = c(0, coef(bs_fit, 1))
}
bs_beta = rbind(c(0, 0), bs_beta, coef(bs_fit, 2))
plot(0, 0,
xlab = expression(beta[1]),
ylab = expression(beta[2]),
main = substitute(paste(rho,"=",r), list(r=rho)),
xlim = c(0, 6),
ylim = c(-1, 3),
type = "n")
par(lwd = 3, cex = 1)
points(ls_beta, col = "black", pch = 16)
lines(ridge_beta, col = "red")
lines(lasso_beta, col = "green")
lines(pcr_beta, col = "purple")
lines(pls_beta, col = "orange")
lines(bs_beta, col = "blue")
abline(h=0, lty = 2)
abline(v=0, lty = 2)
legend(4.8, 3,
c("Ridge", "Lasso", "PCR", "PLS", "Best Subset", "Least Squares"),
col = c("red", "green", "purple", "orange", "blue", "black"),
lty = c(1,1,1,1,1,NA),
pch =c(NA,NA,NA,NA,NA, 16),
box.col = "white",
box.lwd = 0,
bg = "transparent")
dev.off()
# Replay the above whole process
# then draw the other one with rho = -0.5
png("rho_-05.png", width = 640, height = 480)
plot(0, 0,
xlab = expression(beta[1]),
ylab = expression(beta[2]),
main = substitute(paste(rho,"=",r), list(r=rho)),
xlim = c(0, 6),
ylim = c(-1, 3),
type = "n")
par(lwd = 3, cex = 1)
points(ls_beta, col = "black", pch = 16)
lines(ridge_beta, col = "red")
lines(lasso_beta, col = "green")
lines(pcr_beta, col = "purple")
lines(pls_beta, col = "orange")
lines(bs_beta, col = "blue")
abline(h=0, lty = 2)
abline(v=0, lty = 2)
legend(4.8, 3,
c("Ridge", "Lasso", "PCR", "PLS", "Best Subset", "Least Squares"),
col = c("red", "green", "purple", "orange", "blue", "black"),
lty = c(1,1,1,1,1,NA),
pch =c(NA,NA,NA,NA,NA, 16),
box.col = "white",
box.lwd = 0,
bg = "transparent")
dev.off()
rho <- 0.5  # correlation
n <- 100 # number of sample
beta <- as.vector(c(4, 2)) # coefficient
Sigma <- matrix(c(1, rho,
rho, 1), 2, 2)
X <- mvrnorm(n, c(0, 0), Sigma)
Y <- X %*% beta
# I. Least Squared
ls_fit <- lm(Y ~ 0 + X)
ls_beta <- coef(ls_fit)
ls_beta <- as.matrix(t(ls_beta))
# II. Lasso
# Using package glmnet
# Try different lamdas
grid_search = 10^seq(10, -2, length = 100)
library(glmnet)
lasso_fit = glmnet(X, Y, alpha = 1, lambda = grid_search)
lasso_beta = as.matrix(lasso_fit$beta)
lasso_beta = t(lasso_beta)
attr(lasso_beta, "dimnames") = list(NULL,
c("X1","X2"))
# III. Ridge
ridge_fit = glmnet(X, Y, alpha = 0, lambda = grid_search)
ridge_beta = as.matrix(ridge_fit$beta)
ridge_beta = t(ridge_beta)
attr(ridge_beta, "dimnames") = list(NULL,
c("X1", "X2"))
# IV. PCR
# Using package: pls
library(pls)
pcr_fit = pcr(Y ~ X, scale = FALSE)
pcr_beta = pcr_fit$coefficients
pcr_beta = rbind(c(0, 0), pcr_beta[,,1], pcr_beta[,,2])
# V. PLS
pls_fit = plsr(Y ~ X, scale = FALSE)
pls_beta = pls_fit$coefficients
pls_beta = rbind(c(0, 0), pls_beta[,,1], pls_beta[,,2])
# VI. Best Subset
# Using package leaps
library(leaps)
bs_fit = regsubsets(x = X, y = Y, intercept = FALSE)
if (summary(bs_fit)$which[1, 1]){
bs_beta = c(coef(bs_fit, 1), 0)
} else {
bs_beta = c(0, coef(bs_fit, 1))
}
bs_beta = rbind(c(0, 0), bs_beta, coef(bs_fit, 2))
attr(bs_beta, "dimnames") = list(NULL,
c("X1","X2"))
# Final: Draw the Graph
# when rho = 0.5
png("rho_05.png", width = 640, height = 480)
plot(0, 0,
xlab = expression(beta[1]),
ylab = expression(beta[2]),
main = substitute(paste(rho,"=",r), list(r=rho)),
xlim = c(0, 6),
ylim = c(-1, 3),
type = "n")
par(lwd = 3, cex = 1)
points(ls_beta, col = "black", pch = 16)
lines(ridge_beta, col = "red")
lines(lasso_beta, col = "green")
lines(pcr_beta, col = "purple")
lines(pls_beta, col = "orange")
lines(bs_beta, col = "blue")
abline(h=0, lty = 2)
abline(v=0, lty = 2)
legend(4.8, 3,
c("Ridge", "Lasso", "PCR", "PLS", "Best Subset", "Least Squares"),
col = c("red", "green", "purple", "orange", "blue", "black"),
lty = c(1,1,1,1,1,NA),
pch =c(NA,NA,NA,NA,NA, 16),
box.col = "white",
box.lwd = 0,
bg = "transparent")
dev.off()
